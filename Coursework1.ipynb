{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Natural Language Processing Assignment1 1951882<br>\n",
    "Python Code & Report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "from nltk import *\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "with open(\"signal-news1.jsonl\", 'r') as f:\n",
    "    # Read .json file and Lowercase the text\n",
    "    text = [json.loads(line.lower()[:]) for line in f]\n",
    "\n",
    "with open(\"positive-words.txt\") as f:\n",
    "    positive_words = f.read()\n",
    "\n",
    "with open(\"negative-words.txt\") as f:\n",
    "    negative_words = f.read()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part A. Text Preprocessing Q1 a) - d)<br><br>\n",
    "Firstly, all the texts are reduced to lowercase directly when opening the json file. Since there are four steps to preprocess the texts, each step will return a processed text. Therefore, to avoid writing a similar code after each process, I define a function called **<font>parse_and_clean</font>**. It will return a list which stores the preprocessed strings. In this function, the matched patterns are substituted by space and then ‘None’ value in the strings are filtered. <br><br>\n",
    "To clean the texts, four patterns are defined to be used in matching target strings. Before removing the non-alphanumeric characters, URLs are removed firstly in order not to break the URLs’ format. After this, the remaining steps are taken. And the final text is stored in a list text_remove4. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to parse and clean the texts by using regular expressions\n",
    "def parse_and_clean(pattern, signal_news1):\n",
    "    text_remove = list(filter(None, [re.sub(pattern, r'', str(line)) for line in signal_news1]))\n",
    "    return text_remove\n",
    "\n",
    "\n",
    "# Step1. Remove the URL\n",
    "pattern1 = re.compile(r'http[s]?://(?:[a-zA-Z0-9]|[$-_@.&+]|(?:%[0-9a-zA-Z][0-9a-zA-Z]))+')\n",
    "text_remove1 = parse_and_clean(pattern1, text)\n",
    "\n",
    "# Step2 Remove non-alphanumeric characters and spaces by regular expression ([^\\sa-zA-Z0-9])\n",
    "pattern2 = re.compile(r'[^\\sa-zA-Z0-9]')\n",
    "text_remove2 = parse_and_clean(pattern2, text_remove1)\n",
    "\n",
    "# Step3 Remove words with only one character\n",
    "pattern3 = re.compile(r'(\\b[a-zA-Z]\\b)')\n",
    "text_remove3 = parse_and_clean(pattern3, text_remove2)\n",
    "\n",
    "# Step4 Remove numbers that are fully made of digits\n",
    "pattern4 = re.compile(r'(\\b\\d+\\b)')\n",
    "text_remove4 = parse_and_clean(pattern4, text_remove3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part A. Text Preprocessing Q2<br><br>\n",
    "To reduce the vocabulary size, texts need to be lemmatized. Before doing so, a POS tagger is used to for classification. To identify the parts-of-speech of a given word, **<font>word_tokenize()</font>** function is used to split the words in each sub-list of **<font>text_removed4</font>** into tokens. Then, **<font>nltk.pos_tag</font>** function is taken. Results give a set of tuples with two elements: first one is token and the second one is the corresponding tagger. Therefore, a list called **<font>tagged</font>** is used to store these tuples. Since the tagger is pretrained from Penn Treebank project, a **<font>get_wordnet_pos</font>** function is defined to map the treebank tags to WordNet POS names. It is accomplished by identifying the first letter of the tag. As a result, lemmas are stored in a list called **<font>lemma</font>** and taggers are removed after POS tagging. After this normalization, the final **<font>tagged</font>** consists of sub-lists, each sub-list represents a news story and words in the news story are lemmatized. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use pos_tagger to tag word in news\n",
    "tagged = []\n",
    "\n",
    "for items in text_remove4:\n",
    "    tagged.append(pos_tag(word_tokenize(items)))\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemma = []\n",
    "for lists in tagged:\n",
    "    for n in range(len(lists)):\n",
    "        wordnet_pos = get_wordnet_pos(lists[n][1]) or wordnet.NOUN\n",
    "        lists[n] = list(lists[n])  # Change the tuple into list in order to make changes\n",
    "        lists[n][0] = wnl.lemmatize(lists[n][0], pos=wordnet_pos)  # Lemmatize the words according to POS tagging\n",
    "        lemma.append(wnl.lemmatize(lists[n][0], pos=wordnet_pos))  # Append the lemmatized words into a lemma list\n",
    "        lists[n].pop()  # Remove the tag\n",
    "        lists[n] = ''.join(lists[n])  # Convert the type of words from list to strings\n",
    "\n",
    "print(\"Here are the lemmatized words: \", lemma)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part B. N-grams Q1 & Q2<br><br>\n",
    "According to the lemmas that obtained in the above, the **<font>len()</font>** is used to calculate the number of tokens in **<font>lemma</font>**. Then, **<font>set()</font>** is used to avoid duplicated words in **<font>lemma</font>** and its length is the size of vocabulary. For the top 25 trigrams of the corpus, words are stored in trigram type by using **<font>trigrams()</font>**. According to the frequency distribution, the 25 most common trigrams are obtained by using **<font>most_common(25)</font>** function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Q1. Number of tokens and vocabulary size\n",
    "\n",
    "N = len(lemma)\n",
    "V = len(set(lemma))\n",
    "\n",
    "print(\"Number of tokens (N) is \", N)\n",
    "print(\"Vocabulary size (V) is \", V)\n",
    "\n",
    "\n",
    "# Q2. List trigrams and sort it by number of occurrences\n",
    "trigram = nltk.trigrams(lemma)\n",
    "tri_freqdist = nltk.FreqDist(trigram)\n",
    "print(\"Here is the list of top 25 trigrams: \\n\", tri_freqdist.most_common(25))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part B. N-grams Q3 <br><br>\n",
    "To compute the number of positive and negative word counts in the corpus, words in **<font>lemma</font>** are restored in the type of unigram. According to the frequency distribution of this unigram by using **<font>nltk.freqdist</font>**, each token and its corresponding frequency is obtained. Therefore, this problem can be solved by matching the keys of **<font>nltk.freqdist</font>** with the positive words and negative words separately. Each time when the match successes, the number of positive word (**<font>num_positive</font>**) or negative word (**<font>num_negative</font>**) increases by one. After traversing all the tokens in the corpus, computation is done. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a frequency distribution of an uni-gram\n",
    "uni_freqdist = nltk.FreqDist(lemma)\n",
    "\n",
    "positive_words_list = positive_words.splitlines()\n",
    "negative_words_list = negative_words.splitlines()\n",
    "\n",
    "# Count\n",
    "num_positive = 0\n",
    "num_negative = 0\n",
    "\n",
    "for k, v in uni_freqdist.items():\n",
    "    if k in positive_words_list[:]:\n",
    "        num_positive += 1\n",
    "\n",
    "    if k in negative_words_list[:]:\n",
    "        num_negative += 1\n",
    "\n",
    "print(\"Number of positive word counts in the corpus: \", num_positive)\n",
    "print(\"Number of negative word counts in the corpus: \", num_negative)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part B. N-grams Q4<br><br>\n",
    "For the question4, **<font>tagged</font>** is reconsidered to be used because we focus on the news story entirely, rather than the single words in it. Here, a **<font>count_pos_neg</font>** function is defined to calculate the number of positive words (**<font>pos</font>**) as well as number of negative words (**<font>neg</font>**) that occurred in each story. Therefore, each story has a list **<font>[pos, neg]</font>** which contains two values, **<font>pos</font>** and **<font>neg</font>**. Then, a simple comparison between them is done as follow. Each time when the **<font>pos</font>** is greater than **<font>neg</font>**, the number of news with more positive words (**<font>num_more_positive_news</font>**) is increased by one. Otherwise, the number of news with more negative words (**<font>num_more_negative_news</font>**) increases. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to count the positive words and negative words in each sublist\n",
    "def count_pos_neg(item):\n",
    "    i = 0\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    while i < len(item):\n",
    "        if item[i] in positive_words_list[:]:\n",
    "            pos += 1\n",
    "\n",
    "        if item[i] in negative_words_list[:]:\n",
    "            neg += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return [pos, neg]\n",
    "\n",
    "\n",
    "num_more_positive_news = 0\n",
    "num_more_negative_news = 0\n",
    "\n",
    "# Compare the number of positive words and negative words\n",
    "for lists in tagged:\n",
    "    count = count_pos_neg(lists)\n",
    "\n",
    "    if count[0] > count[1]:\n",
    "        num_more_positive_news += 1\n",
    "\n",
    "    if count[0] < count[1]:\n",
    "        num_more_negative_news += 1\n",
    "\n",
    "print(\"Number of positive news stories: \", num_more_positive_news)\n",
    "print(\"Number of negative news stories: \", num_more_negative_news)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part C. Language Models Q1<br><br>\n",
    "To compute the language model for trigrams, the corpus **<font>tagged</font>** is separated into two sets (train set: **<font>train_tagged</font>** and test set: **<font>test_tagged</font>**). Here, **<font>train_tagged</font>** contains the first 16,000 rows of the corpus, that is the first 16,000 items in the **<font>tagged</font>** list.  Then, rest of items in the **<font>tagged</font>** are testing data for the model. To begin with, a placeholder for model is built by using default dictionary because it can automatically create an entry for the new key, corresponding to a default value. In this model, lists in **<font>train_tagged</font>** are split into trigrams with the help of NLTK. Then, by defining w1, w2, w3 in each trigram, frequency of each combination (w1_w2 and w1_w2_w3) occurred in it are calculated. The numerator is the total counts of w3 together with w1 and w2 in **<font>train_tagged</font>**, while the denominator is the occurrence of the two proceeding words of w3 (w1 and w2 only) in **<font>train_tagged</font>**.<br><br>\n",
    "However, there are too many zeros in calculation because of the sparsity of data. Add-one estimation of probabilities is taken. That is, for each occurrence of w1_w2_w3, frequency adds 1. For each occurrence of the proceeding words (w1_w2), frequency adds V (**<font>len(add_v_train)</font>**), which is the total number of unique bigrams in the training corpus. V is calculated by splitting **<font>tagged</font>** into bigrams, and the total number of keys in the bigram frequency distribution is the value. Further, since the denominator will be much greater than the numerator, the probabilities may end up with a floating-point underflow. Thus, log space is taken instead. <br><br>\n",
    "After obtaining the language model, a sentence which starts with “ be this ” is generated instead of “is this” because ‘is’ is lemmatized as ‘be’ in the previous steps. The effect of this difference is slight because ‘this’ can only followed by ‘is’ in common sense. As a result, the sentence is generated word by word. By sorting the dictionary which contains all possible next words in a descending order, the first one will be appended into the text. Then, same method is applied for the next possible words. Once the length of the text reaches 10, the loop will stop. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Q1. Compute language models for trigrams and\n",
    "\n",
    "train_tagged = tagged[:16000]\n",
    "test_tagged = tagged[16000:]\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurrence\n",
    "for lists in train_tagged:\n",
    "    for w1, w2, w3 in trigrams(lists, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "\n",
    "# Number of the unique bi-grams (V), which will be added in the denominator\n",
    "add_v_train = []\n",
    "for lists in train_tagged:\n",
    "    bigram_train = FreqDist(bigrams(lists))\n",
    "    for k, v in bigram_train.items():\n",
    "        if k not in add_v_train:\n",
    "            add_v_train.append(k)\n",
    "print(\"Total number of unique bigrams:\", len(add_v_train))\n",
    "\n",
    "\n",
    "# Transforms the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        numerator = model[w1_w2][w3] + 1\n",
    "        denominator = total_count + len(add_v_train)\n",
    "        model[w1_w2][w3] *= int(math.log(numerator) - math.log(denominator))  # Transform the probability into log\n",
    "\n",
    "# Produce a sentence of 10 words by appending the most likely next word each time\n",
    "text = [\"be\", \"this\"]\n",
    "EOS = False  # End-of-Sentence\n",
    "\n",
    "while not EOS:\n",
    "    dic = sorted(dict(model[tuple(text[-1:])]).items(), key=lambda d: d[1], reverse=True)\n",
    "    text.append(dic[0])\n",
    "    if len(text) == 10:\n",
    "        EOS = True\n",
    "print(' '.join([t for t in text if t]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PartC. Language Models Q2<br><br>\n",
    "According to the language model obtained in Q1, perplexity of this model is calculated based on the testing dataset (**<font>test_tagged</font>**). To calculate it, numerator and denominator are obtained in the same procedure as above. Since add-1 smoothing has been adopted on the model, probability of each list in the **<font>test_tagged</font>** can be obtained directly. Here, pi represents the sum of each probability in the LM.  And the perplexity is calculated based on the equation given in the lecture slides.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Q2. Compute the perplexity\n",
    "pi = 0\n",
    "perplexity = 0\n",
    "\n",
    "for lists in test_tagged:\n",
    "    for w1, w2, w3 in trigrams(lists, pad_right=True, pad_left=True):\n",
    "        for w1_w2 in model:\n",
    "            for w3 in model[w1_w2]:\n",
    "                pi += model[w1_w2][w3]\n",
    "                perplexity += ((1 / pi) ** len(lists))\n",
    "\n",
    "print(perplexity)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}